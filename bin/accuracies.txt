Here, we list the accuracies of the pre-trained networks provided within this repository.

netw_spikey.msgpack : Trained with source/SNABs/mnist/python/mnist_dnn_spikey.py
    Trained for 25 epochs with Adam optimizer, lr of 0.001, 
    l2 regularization of 0.001. 
Epoch 25/25
60000/60000 [==============================] - 1s 20us/step - loss: 0.4017 - acc: 0.8941 - val_loss: 0.3826 - val_acc: 0.9013
Test loss: 0.38262572689056396
Test accuracy: 0.9013


netw_relu_100_100.msgpack : Trained with ReLU in Output Layer, Mean squared error, 784-100-100-10
Epoch 25/25
60000/60000 [==============================] - 2s 26us/step - loss: 0.0113 - acc: 0.9814 - val_loss: 0.0114 - val_acc: 0.9790
Test loss: 0.01140992940068245
Test accuracy: 0.979

netw_softmax_100_100.msgpack: Trained with softmax in Output Layer, Mean squared error, 784-100-100-10
Epoch 25/25
60000/60000 [==============================] - 2s 28us/step - loss: 0.0570 - acc: 0.9953 - val_loss: 0.1122 - val_acc: 0.9795
Test loss: 0.1122096147596836
Test accuracy: 0.9795

netw_double_cnn.msgpack: Trained with softmax in Output Layer, He initialization in non-Output Layer
Trained with categorical crossentropy loss and AdaDelta Optimizer
Epoch 100/100
469/469 [==============================] - 13s 28ms/step - loss: 0.1357 - accuracy: 0.9627 - val_loss: 0.1351 - val_accuracy: 0.9617
Test loss: 0.13510388135910034
Test accuracy: 0.9617000222206116

netw_cnn_pool.msgpack: Trained with softmax in Output Layer, He initialization in non-Output Layer
Trained with categorical crossentropy loss and AdaDelta Optimizer
Epoch 100/100
469/469 [==============================] - 2s 3ms/step - loss: 0.2406 - accuracy: 0.9324 - val_loss: 0.2277 - val_accuracy: 0.9371
Test loss: 0.22773444652557373
Test accuracy: 0.9370999932289124

From Larmarck_ML
sequential_network_60.msgpack: 
sequential_network_65.msgpack: 96.76
sequential_network_250.msgpack: 97.53% (Test)
sequential_network_topAcc.msgpack: 97.71% (Test)
